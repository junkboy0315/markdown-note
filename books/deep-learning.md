# ディープラーニングのしくみがわかる数学入門

[[toc]]

## 超基本

### 賢い動作とは

- 今までのシステムは、データを人間が読んでルールを導く。
- これからのシステムは、人工知能がデータを読んでルールを導く。

#### 交差検証

訓練データと検証データ（テストデータ）を入れ替えることで、精度を確認すること。例えば、ランダムに選んだり、偶数日のデータを訓練データ・奇数日のデータをテストデータにするなど。

#### 最適化問題

（例えば正解率などが）最大や最小となる状態を求める問題のこと。

#### 効率的な探索

最適化問題では、データ量が増えても短い時間で探索する必要があり、以下のような探索方法が使われる。

- 単純増加する関数　 → 　二分探索など
- ランダムに変化する関数　 → 　遺伝的アルゴリズムなど

### 機械学習の考え方

関数 --- 入力を受け取り、何らかの変換を行い、出力する。ブラックボックスのようなもの。

#### 機械学習の種類

- **教師あり学習**
  - 個々のデータが「入力」と「正しい出力」のペアとなるようにする
  - データを「訓練用」と「テスト用」に分ける
  - コンピュータに、訓練用データで学習させ、関数を作らせる
  - 作った関数でテスト用データを処理したときに正解率が高くなるよう、何度も調整してやり直す（どの関数を使うか、パラメータをどう調整するかなど）
- **教師なし学習**
  - 正解がわからない場合、正解を用意できない場合に使う
  - データの特徴を掴むために使う（クラスタリングなど）
- **強化学習**
  - 報酬が最大になるよう、コンピュータに試行錯誤で学習させる

### ニューラルネットワーク

- 人工知能を実現する一つの方法が機械学習
- 機械学習を実現する一つの方法がニューラルネットワーク
- ニューラルネットワークを進化させたのがディープラーニング

#### 動作

![newral](https://cdn-images-1.medium.com/max/479/1*QVIyc5HnGDWTNX3m-nIm9w.png)
[出展](https://medium.com/@curiousily/tensorflow-for-hackers-part-iv-neural-network-from-scratch-1a4f504dfa8)

- ニューロンは入力（=信号）とその「重み」を受け取る。
- 結果がしきい値を超えたら 1 を出力する（発火する）。超えなければ 0 を出力する。
- 入力層、中間層、出力層を持つ
- 上記図の場合、重みを持つ層が 2 層なので、2 層ネットワークと呼ぶ。中間層が増えれば、3 層、4 層・・・ネットワークという。
- まずはじめは、重みはランダムに設定される。その後、訓練データを入れて、正しい結果が得られるよう重みを調整する（**最適化**、**学習**などという）。
- テストデータで正解率を求めて、そのネットワークを評価する
- 訓練データにのみ最適化したネットワークができてしまう**過学習**が起こらないよう、あえて最適化を制限する場合もある

## 数列・統計・確率

### 数列と集合

#### 数列、漸化式

- 数列 --- 数が一列に並んでいるもの。**順番に意味がある。**
- 項 --- 一つの数。初項、第 2 項、第 3 項、、、第$n$項（一般項）
- 一般項 --- 第$n$項の値を、$n$の値だけを使って記述した式
  - $a_n = a + (n-1)d$
- 漸化式 --- 第$n$項の値を、複数の項の関係で記述した式（フィボナッチ数列など）
  - $a_n = a_{n-1} + a_{n-2}$

漸化式は、プログラムのループ処理で書くと簡単。一方、漸化式を一般項に変換するのは難しい場合が多い。

#### 数列の和、シグマ

数列の和はシグマで表される。

数列の和はループ処理で求めてもよいが、公式を活用するとより高速に処理できる。

#### 集合、内包表記

- 集合 --- 数の集まり。**順番に意味がない。** $A, B$ など、大文字アルファベットで表す。
- 有限集合 --- $\left\{1,3,5,7\right\}$など限りがある集合
- 無限集合 --- `整数`など限りがない集合。`{n|nは整数}`などの表現をする。

#### 平均、分散、標準偏差

| 名前                         | 表現       | 説明                                     |
| ---------------------------- | ---------- | ---------------------------------------- |
| 算術平均                     | $\mu$      | NumPy では`mean`、こちらを使うことが多い |
| 加重平均                     | $\mu$      | NumPy では`average`                      |
| 分散(Variance)               | $\sigma^2$ | 「平均との差の 2 乗」の総計              |
| 標準偏差(Standard Deviation) | $\sigma$   | 分散の平方根                             |

#### データの標準化（Z スコア正規化）

平均を 0、分散を 1 に合わせること。標準化された各データ（$z_n$）は、下記の式で求めることができる。

$$
z_n = \frac{x_n - \mu}{\sigma}
$$

- $x_n$ --- 標準化前の各データ
- $\mu$ --- 平均
- $\sigma$ --- 標準偏差

#### データの分布

データの分布を考えることはとても大切。ヒストグラム（度数分布表）などを使う。

- 正規分布（normal distribution）
- 一様分布（uniform distribution）
- 二項分布（binomial distribution）

### 確率

#### 確率の基本

「試行」を行い「事象」が起こる。

事象 A が起こる確率 = 事象 A が起こるパターン数 / **試行によって起こりうる全パターン数**

事象 A が起こる確率を$P(A)$と表す。

- 数学的確率 --- 計算で求めた確率
- 統計的確率 --- 実際に試行して求めた確率

#### 確率変数と確率分布

- 確率変数（$X$）
  - 確率的に決まる値のこと。確率に従っていろいろな値をとる変数のこと。
  - 離散型と連続型がある
  - e.g. サイコロを振ったときに出る目$X$は、確率変数である
- 確率分布
  - 「確率変数の取りうる値」と「その値をとる確率」の対応のこと
  - 離散型では表（確率分布表という）で、連続型ではグラフで表現することが多い

確率によって値が変動するものを「確率変数 $X$」とおき、その確率分布を調べることで、さまざまな物事を「確率による重み」をつけて計算することが可能になる

#### 確率分布表の例

|          |       |       |       |     |       |
| -------- | ----- | ----- | ----- | --- | ----- |
| 確率変数 | $x_1$ | $x_2$ | $x_3$ | ... | $x_n$ |
| 確率     | $p_1$ | $p_2$ | $p_3$ | ... | $p_n$ |

#### 確率変数の期待値（expected value）

確率変数の期待値 = 確率変数の平均値

離散型の場合は下記の式で計算する。

$$
E[X] = \mu = x_1p_1+x_2p_2+...+x_np_n = \sum_{k=1}^n x_k p_k
$$

連続型確率変数の場合は積分により計算する（省略）。

#### 期待値の性質

$C$は定数、$X,Y$は確率変数

- $E[C] = C$
- $E[X+C] = E[X] + C$
- $E[kX] = kE[X]$
- $E[X+Y] = E[X] + E[Y]$ 　※$X,Y$が独立でない場合でも成り立つ

#### 確率変数の分散

離散型の場合は下記の式で計算する。

$$
V[X] = \sigma^2 = (x_1-\mu)^2p_1+ (x_2-\mu)^2p_2+...+ (x_n-\mu)^2p_n = \sum_{k=1}^n (x_k - \mu)^2 p_k
$$

連続型確率変数の場合は積分により計算する（省略）

なお、下記の公式を使えば、分散を期待値から算出することができる。

$$
V[X] = E[X^2] - (E[X])^2
$$

#### 分散の性質

$C$は定数、$X,Y$は確率変数

- $V[C] = 0$
- $V[X+C] = V[X]$
- $V[kX] = k^2V[X]$
- $V[X+Y] = V[X] + V[Y]$ 　※$X,Y$が独立である場合のみ成り立つ

#### 独立

複数の事象が互いに独立である、とは、ある事象が他の事象に影響しないこと

#### 同時確率

独立した 2 つの事象 $A,B$ がある時、

- これらが同時に起こる事象を、事象 $A, B$ の**積事象**といい、$A \cap B$ と表す。
- これらが同時に起こる確率を**同時確率**といい、$P(A \cap B)$ と表す。

同時確率は下記の通り計算する。

$P(A \cap B) = P(A) \times P(B)$

#### 条件付き確率

[参考](https://bellcurve.jp/statistics/course/6438.html)

独立していない事象$A, B$があり、事象$A$が起こることが確定しているときに、**事象$B$が起こる確率**のこと。
Venn diagram で考えるとわかりやすい。

以下、

- $A$：時系列的に先に位置する確率変数（条件、原因）
- $B$：時系列的に後に位置する確率変数（結果）

として表記する。

$$
P(B \mid A) = \frac{P(A \cap B)}{P(A)}
$$

上記のように、条件付き確率では、原因（≒ 条件）を右側に記載する。

`P(Bである確率|Aであったとき)`

#### 確率の乗法定理

[参考](https://bellcurve.jp/statistics/course/6442.html)

独立していない事象$A, B$があるとき、**事象$A, B$が続けて起こる確率**を計算するには、先の式を変形させて導いた「確率の乗法定理」をつかう。

$$
P(A \cap B) = P(A)P(B \mid A)
$$

#### ベイズの定理

[参考動画](https://www.youtube.com/watch?v=oUN_GhB00fU)

$$
P(A \mid B) = \frac{P(B|A)P(A)}{P(B)}
$$

条件付き確率に関する定理の一つ。通常は「原因」から「結果」を探るのに対し、「結果」から「原因」を逆方向に探りたいときに、ベイズの定理を使う。

- $P(A)$ --- 事前確率 まだ何も情報がないときに、既に知っている確率のこと
- $P(A|B)$ --- 事後確率　$B$という情報が**後から**加わる事により、$A$である確率をより絞りんだもの

例）罹患率 0.01%の病気の検査の精度が以下の場合において、陽性と診断されたときに本当に罹患している確率は？

|        | 陽性 | 陰性 |
| ------ | ---- | ---- |
| 罹患   | 98%  | 2%   |
| 非罹患 | 20%  | 80%  |

- 罹患しているとき（原因）に、陽性と判断される（結果）可能性 => 条件付き確率
- 陽性と判断された（結果）ときに、罹患している（原因）可能性 => ベイズの定理

```txt
P(罹|陽) = { P(陽|罹)\*P(罹) } / P(陽)

P(陽|罹) = 0.98
P(罹) = 0.0001
P(陽) = (0.0001 * 0.98) + (0.9999 * 0.20)

P(罹|陽) = 0.05 %
```
